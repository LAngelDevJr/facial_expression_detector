{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90529931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CameraProcessor:\n",
    "    def __init__(self):\n",
    "        self.landmarks_detector = FacialLandmarksDetector()\n",
    "        self.flow_tracker = OpticalFlowTracker()\n",
    "        self.keyframe_detector = KeyFrameDetector()\n",
    "        \n",
    "        # Buffers para la ventana deslizante\n",
    "        self.frames_buffer = deque(maxlen=WINDOW_SIZE)\n",
    "        self.landmarks_buffer = deque(maxlen=WINDOW_SIZE)\n",
    "        self.movement_values = deque(maxlen=WINDOW_SIZE)\n",
    "        \n",
    "        # Almacenamiento de keyframes\n",
    "        self.keyframes = []\n",
    "        self.keyframe_indices = []\n",
    "    \n",
    "    def process_camera(self, camera_index=0, output_folder=\"output_keyframes_camera\", display=True, use_texture_maps=False, max_time=None):\n",
    "        \"\"\"\n",
    "        Procesa el video de una cámara en vivo para detectar expresiones faciales\n",
    "        \n",
    "        Args:\n",
    "            camera_index: Índice de la cámara (0 para la cámara predeterminada)\n",
    "            output_folder: Carpeta donde guardar resultados\n",
    "            display: Si True, muestra visualización\n",
    "            use_texture_maps: Si True, usa mapas de textura para mejorar el resumen\n",
    "            max_time: Tiempo máximo en segundos para grabar (None para continuar hasta presionar ESC)\n",
    "            \n",
    "        Returns:\n",
    "            Si use_texture_maps=False: Tupla (keyframes, keyframe_indices)\n",
    "            Si use_texture_maps=True: Tupla (keyframes, keyframe_indices, texture_maps)\n",
    "        \"\"\"\n",
    "        # Crear carpeta de salida si no existe\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Abrir la cámara\n",
    "        cap = cv2.VideoCapture(camera_index)\n",
    "        if not cap.isOpened():\n",
    "            print(\"⚠️ Error al abrir la cámara\")\n",
    "            return ([], []) if not use_texture_maps else ([], [], {})\n",
    "        \n",
    "        # Parámetros de la cámara\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if fps <= 0:  # Si no se puede detectar FPS, usar un valor predeterminado\n",
    "            fps = 30\n",
    "        print(f\"FPS: {fps}\")\n",
    "        \n",
    "        # Parámetros de procesamiento\n",
    "        frame_skip = max(1, int(fps / 30))  # Procesar solo 30 frames por segundo como máximo\n",
    "        print(f\"Procesando 1 de cada {frame_skip} frames para optimizar velocidad\")\n",
    "        \n",
    "        frame_count = 0\n",
    "        global_keyframe_count = 0\n",
    "        \n",
    "        # Para video de salida\n",
    "        summary_frames = []\n",
    "        temporal_indices = []\n",
    "        \n",
    "        # Conjunto para rastrear frames ya seleccionados\n",
    "        frames_ya_seleccionados = set()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        end_time = None if max_time is None else start_time + max_time\n",
    "        \n",
    "        # Reiniciar buffers\n",
    "        self.frames_buffer.clear()\n",
    "        self.landmarks_buffer.clear()\n",
    "        self.movement_values.clear()\n",
    "        self.keyframes = []\n",
    "        self.keyframe_indices = []\n",
    "        \n",
    "        print(\"Iniciando captura de cámara. Presiona ESC para detener.\")\n",
    "        \n",
    "        # Bucle principal de procesamiento de la cámara\n",
    "        while cap.isOpened():\n",
    "            # Comprobar si hemos alcanzado el tiempo máximo\n",
    "            if end_time is not None and time.time() > end_time:\n",
    "                print(f\"Tiempo máximo alcanzado ({max_time} segundos)\")\n",
    "                break\n",
    "            \n",
    "            # Optimización: Saltar frames para acelerar procesamiento\n",
    "            if frame_count % frame_skip != 0:\n",
    "                ret = cap.grab()  # Solo avanzar sin decodificar el frame\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame_count += 1\n",
    "                continue\n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Actualizar buffers\n",
    "            self.frames_buffer.append(frame.copy())\n",
    "            \n",
    "            # Detectar landmarks faciales y calcular flujo óptico\n",
    "            output_frame, landmarks, movement = self.flow_tracker.track_points(frame, self.landmarks_detector)\n",
    "            self.landmarks_buffer.append(landmarks)\n",
    "            self.movement_values.append(movement)\n",
    "            \n",
    "            # Mostrar información sobre el tiempo de captura\n",
    "            elapsed = time.time() - start_time\n",
    "            mins, secs = divmod(elapsed, 60)\n",
    "            time_info = f\"Tiempo: {int(mins)}:{int(secs):02d}\"\n",
    "            cv2.putText(output_frame, time_info, (10, output_frame.shape[0] - 50), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # Detectar keyframes cuando hay suficientes frames en el buffer\n",
    "            if len(self.frames_buffer) == WINDOW_SIZE:\n",
    "                # Verificar si hay un pico de movimiento que indique un keyframe\n",
    "                is_peak = self.flow_tracker.is_key_frame(movement)\n",
    "                \n",
    "                if is_peak:\n",
    "                    # Detectar keyframes usando el detector\n",
    "                    keyframes_detected = self.keyframe_detector.select_key_frames(\n",
    "                        list(self.frames_buffer), \n",
    "                        list(self.landmarks_buffer),\n",
    "                        list(self.movement_values)\n",
    "                    )\n",
    "                    \n",
    "                    if keyframes_detected:\n",
    "                        print(f\"Keyframes detectados en tiempo: {time_info}\")\n",
    "                        \n",
    "                        # Guardar keyframes evitando duplicados\n",
    "                        for i, kf in enumerate(keyframes_detected):\n",
    "                            # Calcular posición temporal real en secuencia\n",
    "                            frame_idx_temporal = frame_count - (WINDOW_SIZE - i) + 1\n",
    "                            \n",
    "                            # Verificar si este frame ya fue seleccionado antes\n",
    "                            if frame_idx_temporal in frames_ya_seleccionados:\n",
    "                                continue  # Saltar este frame si ya fue seleccionado\n",
    "                            \n",
    "                            # Marcar como seleccionado\n",
    "                            frames_ya_seleccionados.add(frame_idx_temporal)\n",
    "                            \n",
    "                            # Guardar keyframe\n",
    "                            keyframe_path = f\"{output_folder}/keyframe_{global_keyframe_count}_tiempo_{elapsed:.2f}s.jpg\"\n",
    "                            cv2.imwrite(keyframe_path, kf)\n",
    "                            \n",
    "                            # Agregar a las listas\n",
    "                            self.keyframes.append(kf)\n",
    "                            self.keyframe_indices.append(frame_idx_temporal)\n",
    "                            summary_frames.append(kf)\n",
    "                            temporal_indices.append(frame_idx_temporal)\n",
    "                            \n",
    "                            global_keyframe_count += 1\n",
    "                            \n",
    "                            # Añadir mensaje a la pantalla\n",
    "                            cv2.putText(output_frame, \"¡EXPRESIÓN DETECTADA!\", (output_frame.shape[1]//2 - 150, 50), \n",
    "                                      cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            # Mostrar frame procesado\n",
    "            if display:\n",
    "                cv2.imshow('Análisis de Expresiones Faciales en Vivo', output_frame)\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == 27:  # ESC para salir \n",
    "                    break\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        # Liberar recursos\n",
    "        cap.release()\n",
    "        if display:\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        print(f\"Procesamiento completado en {time.time() - start_time:.2f} segundos\")\n",
    "        print(f\"Velocidad promedio: {frame_count/(time.time() - start_time):.2f} frames por segundo\")\n",
    "        \n",
    "        # Comprobar si se detectaron keyframes\n",
    "        if not self.keyframes:\n",
    "            print(\"No se detectaron keyframes\")\n",
    "            return ([], []) if not use_texture_maps else ([], [], {})\n",
    "        \n",
    "        print(f\"Se detectaron {len(self.keyframes)} keyframes\")\n",
    "        \n",
    "        # Ordenar frames por índice temporal antes de crear el resumen\n",
    "        if temporal_indices and summary_frames:\n",
    "            frames_con_indices = list(zip(summary_frames, temporal_indices))\n",
    "            frames_con_indices.sort(key=lambda x: x[1])\n",
    "            summary_frames = [frame for frame, _ in frames_con_indices]\n",
    "            temporal_indices = [idx for _, idx in frames_con_indices]\n",
    "        \n",
    "        # Crear video resumen \n",
    "        if output_folder and summary_frames:\n",
    "            self.create_summary_video(summary_frames, output_folder, fps, temporal_indices)\n",
    "        \n",
    "        # Si se solicitan mapas de textura, generarlos ahora\n",
    "        texture_maps = {}\n",
    "        if use_texture_maps and len(self.frames_buffer) > 0 and len(self.landmarks_buffer) > 0:\n",
    "            # Crear carpeta para mapas de textura\n",
    "            texture_folder = os.path.join(output_folder, \"texture_maps\")\n",
    "            os.makedirs(texture_folder, exist_ok=True)\n",
    "            \n",
    "            # Inicializar generador de mapas de textura\n",
    "            texture_generator = TextureMapGenerator(width=400, height=400)\n",
    "            \n",
    "            print(\"Generando mapas de textura...\")\n",
    "            \n",
    "            # Convertir regiones faciales al formato esperado\n",
    "            facial_regions = {\n",
    "                'indices_frente_cejas': self.landmarks_detector.indices['indices_frente_cejas'],\n",
    "                'indices_ojos': self.landmarks_detector.indices['indices_ojos'],\n",
    "                'indices_nariz': self.landmarks_detector.indices['indices_nariz'],\n",
    "                'indices_mejillas_pomulos': self.landmarks_detector.indices['indices_mejillas_pomulos'],\n",
    "                'indices_boca': self.landmarks_detector.indices['indices_boca'],\n",
    "                'indices_mandibula_menton': self.landmarks_detector.indices['indices_mandibula_menton'],\n",
    "                'indices_arrugas': self.landmarks_detector.indices['indices_arrugas'],\n",
    "                'indices_clave': self.landmarks_detector.indices_clave\n",
    "            }\n",
    "            \n",
    "            # Generar mapas de textura\n",
    "            texture_maps = texture_generator.generate_all_texture_maps(\n",
    "                list(self.frames_buffer),\n",
    "                list(self.landmarks_buffer),\n",
    "                facial_regions\n",
    "            )\n",
    "            \n",
    "            # Guardar y visualizar mapas de textura\n",
    "            if texture_maps:\n",
    "                for map_name, texture_map in texture_maps.items():\n",
    "                    map_path = os.path.join(texture_folder, f\"{map_name}.jpg\")\n",
    "                    cv2.imwrite(map_path, texture_map)\n",
    "                    print(f\"Mapa de textura {map_name} guardado en {map_path}\")\n",
    "                \n",
    "                # Visualizar mapas si se solicita\n",
    "                if display:\n",
    "                    try:\n",
    "                        fig = texture_generator.visualize_texture_maps(texture_maps)\n",
    "                        plt.savefig(os.path.join(texture_folder, \"texture_maps_visualization.png\"))\n",
    "                        plt.close(fig)\n",
    "                    except Exception as e:\n",
    "                        print(f\"No se pudieron visualizar los mapas de textura: {str(e)}\")\n",
    "                \n",
    "                # Crear video resumen mejorado\n",
    "                enhanced_video_path = os.path.join(output_folder, \"enhanced_summary.avi\")\n",
    "                texture_generator.create_enhanced_summary_video(\n",
    "                    texture_maps,\n",
    "                    summary_frames,\n",
    "                    enhanced_video_path,\n",
    "                    fps=fps / 2\n",
    "                )\n",
    "                print(f\"Video resumen mejorado creado en {enhanced_video_path}\")\n",
    "        \n",
    "        # Realizar análisis de resultados\n",
    "        if self.keyframes and len(self.keyframes) > 0:\n",
    "            try:\n",
    "                analyzer = ResultAnalyzer()\n",
    "                \n",
    "                # Obtener número aproximado de frames totales\n",
    "                total_frames = frame_count\n",
    "                \n",
    "                # Mostrar distribución de keyframes\n",
    "                plt_dist = analyzer.visualize_keyframe_distribution(self.keyframe_indices, total_frames)\n",
    "                plt_dist.savefig(f\"{output_folder}/keyframe_distribution.png\")\n",
    "                \n",
    "                # Analizar movimiento en keyframes\n",
    "                movement_data = analyzer.analyze_movement_patterns(self.keyframes, self.landmarks_detector)\n",
    "                \n",
    "                if movement_data:\n",
    "                    plt_movement = analyzer.plot_movement_analysis(movement_data)\n",
    "                    plt_movement.savefig(f\"{output_folder}/movement_analysis.png\")\n",
    "                \n",
    "                print(f\"\\nGráficos de análisis guardados en {output_folder}\")\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo realizar análisis avanzado: {str(e)}\")\n",
    "        \n",
    "        # Devolver resultados según el modo\n",
    "        if use_texture_maps:\n",
    "            return self.keyframes, self.keyframe_indices, texture_maps\n",
    "        else:\n",
    "            return self.keyframes, self.keyframe_indices\n",
    "    \n",
    "    def create_summary_video(self, frames, output_folder, original_fps, temporal_indices=None):\n",
    "        \"\"\"Crea un video resumen con transiciones suaves entre keyframes\"\"\"\n",
    "        # Este método es idéntico al de la clase VideoProcessor original\n",
    "        if not frames:\n",
    "            print(\"No hay frames para crear el resumen\")\n",
    "            return\n",
    "        \n",
    "        # Definir el codec y crear objeto VideoWriter\n",
    "        height, width = frames[0].shape[:2]\n",
    "        output_path = f\"{output_folder}/resumen_fluido.avi\"\n",
    "        \n",
    "        # Usar un codec compatible\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "               \n",
    "        if temporal_indices:\n",
    "            # Ordenar frames por índice temporal\n",
    "            frames_con_indices = list(zip(frames, temporal_indices))\n",
    "            frames_con_indices.sort(key=lambda x: x[1])\n",
    "            \n",
    "            # Comprobar espaciado temporal y eliminar frames demasiado cercanos\n",
    "            frames_filtrados = []\n",
    "            indices_filtrados = []\n",
    "            ultimo_indice = -10  # Iniciar con un valor que garantice que el primer frame se incluya\n",
    "            \n",
    "            for frame, indice in frames_con_indices:\n",
    "                # Solo añadir frame si está suficientemente separado del anterior\n",
    "                if indice - ultimo_indice >= 3:\n",
    "                    frames_filtrados.append(frame)\n",
    "                    indices_filtrados.append(indice)\n",
    "                    ultimo_indice = indice\n",
    "            \n",
    "            frames = frames_filtrados\n",
    "            temporal_indices = indices_filtrados\n",
    "        \n",
    "        # Crear video con transiciones suaves\n",
    "        output_fps = original_fps / 2\n",
    "        out = cv2.VideoWriter(output_path, fourcc, output_fps, (width, height))\n",
    "        \n",
    "        # Añadir frames con transiciones\n",
    "        for i in range(len(frames)):\n",
    "            frame_with_text = frames[i].copy()\n",
    "            \n",
    "            # Añadir etiqueta para debug\n",
    "            if temporal_indices and i < len(temporal_indices):\n",
    "                cv2.putText(frame_with_text, f\"FRAME {i+1}/{len(frames)} (Original: {temporal_indices[i]})\", \n",
    "                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            out.write(frame_with_text)\n",
    "            \n",
    "            # Generar transiciones suaves entre frames (excepto el último)\n",
    "            if i < len(frames) - 1:\n",
    "                for t in range(1, 3):  # 2 frames de transición\n",
    "                    alpha = t / 3.0\n",
    "                    transicion = cv2.addWeighted(frames[i], 1-alpha, frames[i+1], alpha, 0)\n",
    "                    out.write(transicion)\n",
    "        \n",
    "        out.release()\n",
    "        print(f\"Video resumen creado en {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
